{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to Rendering an OpenAI Gym Environment Inline\n",
    "The aim of this notebook is to demistify Reinforcement Learning. To expose how simple it really is to get going. Make these systems as accessible to non-AI/ML developers as possible.\n",
    " \n",
    "### Resources\n",
    "- https://gym.openai.com/docs/#environments\n",
    "- https://keras-rl.readthedocs.io/en/latest/agents/overview/\n",
    " \n",
    "### TODO\n",
    "- Refactor this notebook into nicely abstracted utilities that we can use to spin up RL agents with ease for any environment / problem domain.\n",
    "- Create and showcase a custom environment creation and being navigated by the RL agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\programdata\\miniforge3\\lib\\site-packages (21.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement andas (from versions: none)\n",
      "ERROR: No matching distribution found for andas\n",
      "ERROR: Could not find a version that satisfies the requirement rllib3 (from versions: none)\n",
      "ERROR: No matching distribution found for rllib3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\programdata\\miniforge3\\lib\\site-packages (5.3.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\miniforge3\\lib\\site-packages (from plotly) (1.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\programdata\\miniforge3\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: opencv-python in c:\\programdata\\miniforge3\\lib\\site-packages (4.5.3.56)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\programdata\\miniforge3\\lib\\site-packages (from opencv-python) (1.19.5)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\programdata\\miniforge3\\lib\\site-packages (4.5.3.56)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\programdata\\miniforge3\\lib\\site-packages (from opencv-contrib-python) (1.19.5)\n",
      "Requirement already satisfied: av in c:\\programdata\\miniforge3\\lib\\site-packages (8.0.3)\n",
      "Requirement already satisfied: pyvirtualdisplay in c:\\programdata\\miniforge3\\lib\\site-packages (2.2)\n",
      "Requirement already satisfied: EasyProcess in c:\\programdata\\miniforge3\\lib\\site-packages (from pyvirtualdisplay) (0.3)\n",
      "Requirement already satisfied: pyglet in c:\\programdata\\miniforge3\\lib\\site-packages (1.5.21)\n",
      "Requirement already satisfied: ale-py in c:\\programdata\\miniforge3\\lib\\site-packages (0.7.2)\n",
      "Requirement already satisfied: importlib-resources in c:\\programdata\\miniforge3\\lib\\site-packages (from ale-py) (5.2.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\miniforge3\\lib\\site-packages (from ale-py) (1.19.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\programdata\\miniforge3\\lib\\site-packages (from importlib-resources->ale-py) (3.6.0)\n",
      "Requirement already satisfied: pyopengl in c:\\programdata\\miniforge3\\lib\\site-packages (3.1.5)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.4.3-cp39-cp39-win_amd64.whl (7.1 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp39-cp39-win_amd64.whl (52 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\miniforge3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\miniforge3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\programdata\\miniforge3\\lib\\site-packages (from matplotlib) (1.19.5)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.3.2-cp39-cp39-win_amd64.whl (3.2 MB)\n",
      "Requirement already satisfied: six in c:\\programdata\\miniforge3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Installing collected packages: pillow, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.2 matplotlib-3.4.3 pillow-8.3.2\n",
      "Requirement already satisfied: box2d-kengz in c:\\users\\deanm\\appdata\\roaming\\python\\python39\\site-packages (2.3.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install -U pip\n",
    "!{sys.executable} -m pip install -U andas\n",
    "!{sys.executable} -m pip install -U rllib3\n",
    "!{sys.executable} -m pip install -U plotly\n",
    "!{sys.executable} -m pip install -U opencv-python\n",
    "!{sys.executable} -m pip install -U opencv-contrib-python\n",
    "!{sys.executable} -m pip install -U av\n",
    "!{sys.executable} -m pip install -U pyvirtualdisplay\n",
    "!{sys.executable} -m pip install -U pyglet\n",
    "!{sys.executable} -m pip install -U ale-py\n",
    "!{sys.executable} -m pip install -U pyopengl\n",
    "!{sys.executable} -m pip install -U matplotlib\n",
    "!{sys.executable} -m pip install -U box2d-kengz --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Virtual Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "EasyProcessError",
     "evalue": "start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[WinError 2] The system cannot find the file specified return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\easyprocess\\__init__.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m             self.popen = subprocess.Popen(\n\u001b[0m\u001b[0;32m    169\u001b[0m                 \u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1421\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEasyProcessError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8744/1393124638.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyvirtualdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdisplay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m900\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\pyvirtualdisplay\\display.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown backend: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         self._obj = cls(\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mcolor_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor_depth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\pyvirtualdisplay\\xvfb.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         AbstractDisplay.__init__(\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mPROGRAM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\pyvirtualdisplay\\abstractdisplay.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_retries_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mhelptext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_helptext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprogram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_displayfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"-displayfd\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhelptext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_displayfd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\pyvirtualdisplay\\util.py\u001b[0m in \u001b[0;36mget_helptext\u001b[1;34m(program)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_stdout_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_stderr_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mhelptext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhelptext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\easyprocess\\__init__.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \"\"\"\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\easyprocess\\__init__.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"OSError exception: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moserror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moserror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moserror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mEasyProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"start error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"process was started (pid=%s)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEasyProcessError\u001b[0m: start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[WinError 2] The system cannot find the file specified return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIGymSessionVideo:\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "        self.frames = []\n",
    "    \n",
    "    def renderAndCapture(self, epoch_id):\n",
    "        from PIL import Image\n",
    "        import base64\n",
    "        from io import BytesIO\n",
    "\n",
    "        three_d_rgb_array = self.environment.render(mode='rgb_array')\n",
    "        image = Image.fromarray(three_d_rgb_array, 'RGB')\n",
    "        image_buffer = BytesIO()\n",
    "        image.save(image_buffer, format='PNG')\n",
    "\n",
    "        import numpy as np\n",
    "        import cv2\n",
    "        import io\n",
    "        import os\n",
    "        \n",
    "        video_fps = 30\n",
    "        video_codec = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        video_output = cv2.VideoWriter(f'{epoch_id}.mp4', video_codec, video_fps, image.size)\n",
    "\n",
    "        for frame in self.frames:\n",
    "            video_output.write(frame)\n",
    "\n",
    "        video_output.release()\n",
    "        # Convert the video to codecs web supports.\n",
    "        os.system(f\"ffmpeg -i {epoch_id}.mp4 -vcodec libx264 {epoch_id}.web.mp4\")\n",
    "        \n",
    "        self.frames = []\n",
    "        video = io.open(f'{epoch_id}.web.mp4', 'r+b').read()\n",
    "        encoded_video = base64.b64encode(video)\n",
    "        base64_video = encoded_video.decode('utf-8')\n",
    "        video_tag =f'<video controls loop autoplay width=\"250px\" height=\"200px\"><source src=\"data:video/mp4;base64,{base64_video}\" type=\"video/mp4\" /></video>'\n",
    "        \n",
    "        displayHTML(video_tag)\n",
    "    \n",
    "    def capture(self):\n",
    "        from PIL import Image\n",
    "        import base64\n",
    "        from io import BytesIO\n",
    "\n",
    "        three_d_rgb_array = self.environment.render(mode='rgb_array')\n",
    "        image = Image.fromarray(three_d_rgb_array, 'RGB')\n",
    "        image_buffer = BytesIO()\n",
    "        image.save(image_buffer, format='PNG')\n",
    "        \n",
    "        import numpy as np\n",
    "        import cv2\n",
    "        \n",
    "        im_arr = np.frombuffer(image_buffer.getvalue(), dtype=np.uint8)\n",
    "        self.frames.append(cv2.imdecode(im_arr, flags=cv2.IMREAD_COLOR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIGymSession:\n",
    "    def __init__(self, environment_name):\n",
    "        self.environment_name = environment_name\n",
    "    \n",
    "def start(self, episodes, max_epochs=-1):\n",
    "    import gym\n",
    "    import uuid\n",
    "\n",
    "    session_id = uuid.uuid4()\n",
    "    env = gym.make(self.environment_name)\n",
    "    env_display = OpenAIGymSessionVideo(env)\n",
    "\n",
    "    # For each iteration we want to run.\n",
    "    for episode in range(episodes):\n",
    "        env.reset()\n",
    "        \n",
    "        episodes_id = uuid.uuid4()\n",
    "        current_epoch = 0\n",
    "        # Take an initial random action / step.\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # Run the loop again if the environment is not done.\n",
    "        while not(done):\n",
    "            current_epoch += 1\n",
    "\n",
    "            # Break out of the loop if we have reached max_epochs with no done status.\n",
    "            if max_epochs > -1 and current_epoch >= max_epochs:\n",
    "                # Render last image of this iteration.\n",
    "                env_display.renderAndCapture(episodes_id)\n",
    "                return\n",
    "        \n",
    "            # Take next action / step.\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            env_display.capture()\n",
    "            \n",
    "        # Render last image of this iteration.\n",
    "        env_display.renderAndCapture(episodes_id)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIGymSessionForModel:\n",
    "    def __init__(self, environment_name):\n",
    "        self.environment_name = environment_name\n",
    "    \n",
    "    def start(self, model, episodes=1, max_epochs=-1):\n",
    "        import gym\n",
    "        import uuid\n",
    "\n",
    "        session_id = uuid.uuid4()\n",
    "        env = gym.make(self.environment_name)\n",
    "        env_display = OpenAIGymSessionVideo(env)\n",
    "\n",
    "        # For each iteration we want to run.\n",
    "        for episode in range(episodes):\n",
    "            initial_observation = env.reset()\n",
    "        \n",
    "            episodes_id = uuid.uuid4()\n",
    "            current_epoch = 0\n",
    "            # Take an initial random action / step.\n",
    "            action = model.forward(initial_observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            # Run the loop again if the environment is not done.\n",
    "            while not(done):\n",
    "                current_epoch += 1\n",
    "\n",
    "                # Break out of the loop if we have reached max_epochs with no done status.\n",
    "                if max_epochs > -1 and current_epoch >= max_epochs:\n",
    "                    # Render last image of this iteration.\n",
    "                    env_display.renderAndCapture(episodes_id)\n",
    "                    return\n",
    "                \n",
    "                # Take next action / step.\n",
    "                action = model.forward(observation)\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                env_display.capture()\n",
    "\n",
    "        # Render last image of this iteration.\n",
    "        env_display.renderAndCapture(episodes_id)\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment\n",
    "Our utilities support rendering various OpenAI Gym environments. See https://gym.openai.com/envs/#classic_control for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1;\n",
    "env_name = \"LunarLander-v2\"\n",
    "session = OpenAIGymSession(env_name)\n",
    "\n",
    "session.start(episodes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1\n",
    "env_name = \"CartPole-v1\"\n",
    "session = OpenAIGymSession(env_name)\n",
    "\n",
    "session.start(episodes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# Simple environment taking a discrete and continuious action.\n",
    "class AdditionCompetitionEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AdditionCompetitionEnv, self).__init__()\n",
    "\n",
    "        self.state = 0\n",
    "        # 0) Sutract 1, 1) Add 1\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.state -= 1\n",
    "        elif action == 2:\n",
    "            self.state += 1\n",
    "        \n",
    "        done = self.state <= 200 or self.state >= 200\n",
    "        info = {}\n",
    "        \n",
    "        return self.state, self.state, done, info\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        #self.state = 0\n",
    "        \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducting a Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 756\n",
      "Trainable params: 756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniforge3\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "C:\\ProgramData\\miniforge3\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   185/50000: episode: 1, duration: 2.522s, episode steps: 185, steps per second:  73, episode reward: -341.377, mean reward: -1.845 [-100.000,  7.102], mean action: 1.876 [0.000, 3.000],  loss: 1.598832, mae: 1.011014, mean_q: 1.554121\n",
      "   364/50000: episode: 2, duration: 1.109s, episode steps: 179, steps per second: 161, episode reward: -387.553, mean reward: -2.165 [-100.000,  3.587], mean action: 1.743 [0.000, 3.000],  loss: 21.422327, mae: 1.202801, mean_q: 1.899935\n",
      "   488/50000: episode: 3, duration: 0.793s, episode steps: 124, steps per second: 156, episode reward: -65.967, mean reward: -0.532 [-100.000, 40.776], mean action: 1.548 [0.000, 3.000],  loss: 26.604265, mae: 1.886929, mean_q: 1.912362\n",
      "   777/50000: episode: 4, duration: 1.891s, episode steps: 289, steps per second: 153, episode reward: -153.809, mean reward: -0.532 [-100.000, 35.946], mean action: 1.803 [0.000, 3.000],  loss: 25.640825, mae: 2.505280, mean_q: 2.503996\n",
      "   915/50000: episode: 5, duration: 0.842s, episode steps: 138, steps per second: 164, episode reward: -306.288, mean reward: -2.219 [-100.000,  9.785], mean action: 1.623 [0.000, 3.000],  loss: 21.614662, mae: 3.284259, mean_q: 3.315351\n",
      "  1142/50000: episode: 6, duration: 1.403s, episode steps: 227, steps per second: 162, episode reward: -70.690, mean reward: -0.311 [-100.000, 13.842], mean action: 1.850 [0.000, 3.000],  loss: 24.489735, mae: 4.539265, mean_q: 3.770038\n",
      "  1284/50000: episode: 7, duration: 0.868s, episode steps: 142, steps per second: 164, episode reward:  2.848, mean reward:  0.020 [-100.000,  7.258], mean action: 1.662 [0.000, 3.000],  loss: 22.857885, mae: 6.200907, mean_q: 5.759447\n",
      "  1470/50000: episode: 8, duration: 1.187s, episode steps: 186, steps per second: 157, episode reward: -41.555, mean reward: -0.223 [-100.000, 10.299], mean action: 1.683 [0.000, 3.000],  loss: 24.014061, mae: 7.256724, mean_q: 7.109841\n",
      "  1658/50000: episode: 9, duration: 1.247s, episode steps: 188, steps per second: 151, episode reward: -73.162, mean reward: -0.389 [-100.000, 51.953], mean action: 1.617 [0.000, 3.000],  loss: 20.909903, mae: 8.073171, mean_q: 7.916787\n",
      "  1794/50000: episode: 10, duration: 0.850s, episode steps: 136, steps per second: 160, episode reward: -235.859, mean reward: -1.734 [-100.000, 53.551], mean action: 1.765 [0.000, 3.000],  loss: 14.993760, mae: 9.243347, mean_q: 9.756215\n",
      "  1985/50000: episode: 11, duration: 1.193s, episode steps: 191, steps per second: 160, episode reward: 69.394, mean reward:  0.363 [-100.000, 18.866], mean action: 1.764 [0.000, 3.000],  loss: 17.333557, mae: 10.101777, mean_q: 10.235603\n",
      "  2119/50000: episode: 12, duration: 0.891s, episode steps: 134, steps per second: 150, episode reward: -77.919, mean reward: -0.581 [-100.000, 31.426], mean action: 1.731 [0.000, 3.000],  loss: 19.397409, mae: 11.191614, mean_q: 10.779657\n",
      "  2236/50000: episode: 13, duration: 0.797s, episode steps: 117, steps per second: 147, episode reward: -38.904, mean reward: -0.333 [-100.000,  8.254], mean action: 1.778 [0.000, 3.000],  loss: 17.067238, mae: 11.823080, mean_q: 11.706317\n",
      "  2517/50000: episode: 14, duration: 1.793s, episode steps: 281, steps per second: 157, episode reward: -18.607, mean reward: -0.066 [-100.000, 16.098], mean action: 1.779 [0.000, 3.000],  loss: 17.016182, mae: 12.657046, mean_q: 12.583615\n",
      "  2629/50000: episode: 15, duration: 0.688s, episode steps: 112, steps per second: 163, episode reward: -3.839, mean reward: -0.034 [-100.000, 17.213], mean action: 1.830 [0.000, 3.000],  loss: 20.045788, mae: 14.542974, mean_q: 12.935111\n",
      "  2871/50000: episode: 16, duration: 1.658s, episode steps: 242, steps per second: 146, episode reward: -192.695, mean reward: -0.796 [-100.000,  6.159], mean action: 1.607 [0.000, 3.000],  loss: 13.672626, mae: 14.469240, mean_q: 13.972964\n",
      "  3099/50000: episode: 17, duration: 1.430s, episode steps: 228, steps per second: 159, episode reward: -122.698, mean reward: -0.538 [-100.000,  5.908], mean action: 1.623 [0.000, 3.000],  loss: 11.604387, mae: 15.481505, mean_q: 15.575405\n",
      "  3530/50000: episode: 18, duration: 2.762s, episode steps: 431, steps per second: 156, episode reward: -140.373, mean reward: -0.326 [-100.000,  6.696], mean action: 1.680 [0.000, 3.000],  loss: 11.788507, mae: 16.584568, mean_q: 16.993452\n",
      "  3732/50000: episode: 19, duration: 1.347s, episode steps: 202, steps per second: 150, episode reward: -78.935, mean reward: -0.391 [-100.000, 16.471], mean action: 1.673 [0.000, 3.000],  loss: 9.183489, mae: 17.129356, mean_q: 18.564644\n",
      "  3941/50000: episode: 20, duration: 1.356s, episode steps: 209, steps per second: 154, episode reward: -67.104, mean reward: -0.321 [-100.000, 14.216], mean action: 1.641 [0.000, 3.000],  loss: 10.122107, mae: 17.565475, mean_q: 18.887882\n",
      "  4290/50000: episode: 21, duration: 2.266s, episode steps: 349, steps per second: 154, episode reward: -161.072, mean reward: -0.462 [-100.000,  7.910], mean action: 1.685 [0.000, 3.000],  loss: 10.777664, mae: 18.887283, mean_q: 19.629259\n",
      "  4500/50000: episode: 22, duration: 1.398s, episode steps: 210, steps per second: 150, episode reward: -47.824, mean reward: -0.228 [-100.000, 12.288], mean action: 1.738 [0.000, 3.000],  loss: 14.099786, mae: 19.511457, mean_q: 20.944605\n",
      "  4675/50000: episode: 23, duration: 1.083s, episode steps: 175, steps per second: 162, episode reward: -14.739, mean reward: -0.084 [-100.000,  8.227], mean action: 1.731 [0.000, 3.000],  loss: 10.454342, mae: 19.789755, mean_q: 21.561852\n",
      "  4939/50000: episode: 24, duration: 1.658s, episode steps: 264, steps per second: 159, episode reward: -68.003, mean reward: -0.258 [-100.000,  7.067], mean action: 1.686 [0.000, 3.000],  loss: 11.835802, mae: 19.802635, mean_q: 21.630129\n",
      "  5124/50000: episode: 25, duration: 1.139s, episode steps: 185, steps per second: 162, episode reward: -124.791, mean reward: -0.675 [-100.000,  3.040], mean action: 1.719 [0.000, 3.000],  loss: 12.292074, mae: 20.322243, mean_q: 21.795181\n",
      "  5399/50000: episode: 26, duration: 1.837s, episode steps: 275, steps per second: 150, episode reward: -94.497, mean reward: -0.344 [-100.000,  7.003], mean action: 1.691 [0.000, 3.000],  loss: 10.308106, mae: 20.771343, mean_q: 21.942192\n",
      "  5644/50000: episode: 27, duration: 1.549s, episode steps: 245, steps per second: 158, episode reward: -95.627, mean reward: -0.390 [-100.000,  6.953], mean action: 1.776 [0.000, 3.000],  loss: 12.149479, mae: 20.931250, mean_q: 21.730167\n",
      "  5892/50000: episode: 28, duration: 1.553s, episode steps: 248, steps per second: 160, episode reward: -75.875, mean reward: -0.306 [-100.000,  7.362], mean action: 1.827 [0.000, 3.000],  loss: 8.648980, mae: 21.696856, mean_q: 21.936504\n",
      "  6108/50000: episode: 29, duration: 1.347s, episode steps: 216, steps per second: 160, episode reward: -115.895, mean reward: -0.537 [-100.000,  7.980], mean action: 1.722 [0.000, 3.000],  loss: 9.907881, mae: 22.414780, mean_q: 21.828516\n",
      "  6324/50000: episode: 30, duration: 1.465s, episode steps: 216, steps per second: 147, episode reward: -119.111, mean reward: -0.551 [-100.000,  4.962], mean action: 1.782 [0.000, 3.000],  loss: 8.707794, mae: 22.694401, mean_q: 23.276438\n",
      "  6516/50000: episode: 31, duration: 1.219s, episode steps: 192, steps per second: 158, episode reward: -52.212, mean reward: -0.272 [-100.000,  3.872], mean action: 1.823 [0.000, 3.000],  loss: 9.236194, mae: 23.469885, mean_q: 23.606913\n",
      "  6721/50000: episode: 32, duration: 1.291s, episode steps: 205, steps per second: 159, episode reward: -38.367, mean reward: -0.187 [-100.000, 14.023], mean action: 1.712 [0.000, 3.000],  loss: 11.708820, mae: 24.008013, mean_q: 24.126854\n",
      "  6920/50000: episode: 33, duration: 1.229s, episode steps: 199, steps per second: 162, episode reward: -121.886, mean reward: -0.612 [-100.000,  4.228], mean action: 1.779 [0.000, 3.000],  loss: 7.440644, mae: 24.490116, mean_q: 24.066887\n",
      "  7242/50000: episode: 34, duration: 2.120s, episode steps: 322, steps per second: 152, episode reward: -133.314, mean reward: -0.414 [-100.000,  4.897], mean action: 1.876 [0.000, 3.000],  loss: 6.058965, mae: 25.272961, mean_q: 24.675058\n",
      "  7601/50000: episode: 35, duration: 2.293s, episode steps: 359, steps per second: 157, episode reward: -90.377, mean reward: -0.252 [-100.000, 14.553], mean action: 1.755 [0.000, 3.000],  loss: 6.400751, mae: 26.092926, mean_q: 25.002666\n",
      "  8204/50000: episode: 36, duration: 4.132s, episode steps: 603, steps per second: 146, episode reward: 150.659, mean reward:  0.250 [-13.087, 100.000], mean action: 1.524 [0.000, 3.000],  loss: 6.433336, mae: 27.138968, mean_q: 24.203539\n",
      "  8491/50000: episode: 37, duration: 1.973s, episode steps: 287, steps per second: 145, episode reward: -35.056, mean reward: -0.122 [-100.000,  9.996], mean action: 1.794 [0.000, 3.000],  loss: 9.535398, mae: 28.208078, mean_q: 22.582865\n",
      "  9350/50000: episode: 38, duration: 5.973s, episode steps: 859, steps per second: 144, episode reward: -315.649, mean reward: -0.367 [-100.000, 21.672], mean action: 1.764 [0.000, 3.000],  loss: 7.787763, mae: 28.326069, mean_q: 23.209822\n",
      "  9899/50000: episode: 39, duration: 3.846s, episode steps: 549, steps per second: 143, episode reward: -110.080, mean reward: -0.201 [-100.000,  8.982], mean action: 1.809 [0.000, 3.000],  loss: 8.238815, mae: 28.880766, mean_q: 23.278170\n",
      " 10899/50000: episode: 40, duration: 6.957s, episode steps: 1000, steps per second: 144, episode reward: -50.218, mean reward: -0.050 [-5.330,  4.797], mean action: 1.761 [0.000, 3.000],  loss: 6.003111, mae: 29.309566, mean_q: 24.928484\n",
      " 11899/50000: episode: 41, duration: 7.247s, episode steps: 1000, steps per second: 138, episode reward: -56.145, mean reward: -0.056 [-4.818,  5.273], mean action: 1.792 [0.000, 3.000],  loss: 7.326953, mae: 30.333605, mean_q: 27.315132\n",
      " 12899/50000: episode: 42, duration: 7.460s, episode steps: 1000, steps per second: 134, episode reward: -48.771, mean reward: -0.049 [-4.937,  5.074], mean action: 1.797 [0.000, 3.000],  loss: 6.767338, mae: 31.344254, mean_q: 30.900074\n",
      " 13685/50000: episode: 43, duration: 5.698s, episode steps: 786, steps per second: 138, episode reward: -194.010, mean reward: -0.247 [-100.000,  5.128], mean action: 1.768 [0.000, 3.000],  loss: 4.869778, mae: 32.315929, mean_q: 33.532806\n",
      " 14385/50000: episode: 44, duration: 4.854s, episode steps: 700, steps per second: 144, episode reward: -193.119, mean reward: -0.276 [-100.000,  4.920], mean action: 1.769 [0.000, 3.000],  loss: 7.315836, mae: 33.027111, mean_q: 35.281361\n",
      " 15197/50000: episode: 45, duration: 6.045s, episode steps: 812, steps per second: 134, episode reward: -241.053, mean reward: -0.297 [-100.000,  5.172], mean action: 1.724 [0.000, 3.000],  loss: 6.567880, mae: 33.765194, mean_q: 37.835991\n",
      " 15614/50000: episode: 46, duration: 2.803s, episode steps: 417, steps per second: 149, episode reward: -134.027, mean reward: -0.321 [-100.000,  6.017], mean action: 1.772 [0.000, 3.000],  loss: 9.619580, mae: 34.453533, mean_q: 39.459724\n",
      " 16271/50000: episode: 47, duration: 4.670s, episode steps: 657, steps per second: 141, episode reward: -180.906, mean reward: -0.275 [-100.000,  5.372], mean action: 1.737 [0.000, 3.000],  loss: 10.695163, mae: 34.342602, mean_q: 40.166492\n",
      " 16675/50000: episode: 48, duration: 2.618s, episode steps: 404, steps per second: 154, episode reward: -136.637, mean reward: -0.338 [-100.000,  4.927], mean action: 1.735 [0.000, 3.000],  loss: 8.714485, mae: 34.496548, mean_q: 40.413952\n",
      " 17240/50000: episode: 49, duration: 4.019s, episode steps: 565, steps per second: 141, episode reward: -200.972, mean reward: -0.356 [-100.000,  5.522], mean action: 1.779 [0.000, 3.000],  loss: 8.281573, mae: 34.398502, mean_q: 41.179050\n",
      " 17843/50000: episode: 50, duration: 4.285s, episode steps: 603, steps per second: 141, episode reward: -211.215, mean reward: -0.350 [-100.000,  4.895], mean action: 1.720 [0.000, 3.000],  loss: 8.406525, mae: 34.410759, mean_q: 41.103043\n",
      " 18479/50000: episode: 51, duration: 4.415s, episode steps: 636, steps per second: 144, episode reward: -208.612, mean reward: -0.328 [-100.000,  4.653], mean action: 1.741 [0.000, 3.000],  loss: 8.434900, mae: 34.041321, mean_q: 40.687145\n",
      " 18953/50000: episode: 52, duration: 3.156s, episode steps: 474, steps per second: 150, episode reward: -190.966, mean reward: -0.403 [-100.000,  5.438], mean action: 1.827 [0.000, 3.000],  loss: 5.568978, mae: 33.895874, mean_q: 40.910664\n",
      " 19498/50000: episode: 53, duration: 3.836s, episode steps: 545, steps per second: 142, episode reward: -198.909, mean reward: -0.365 [-100.000,  4.713], mean action: 1.739 [0.000, 3.000],  loss: 6.968601, mae: 33.341412, mean_q: 40.678757\n",
      " 20078/50000: episode: 54, duration: 4.087s, episode steps: 580, steps per second: 142, episode reward: -168.976, mean reward: -0.291 [-100.000,  6.067], mean action: 1.798 [0.000, 3.000],  loss: 10.314813, mae: 33.117935, mean_q: 40.357300\n",
      " 20582/50000: episode: 55, duration: 3.294s, episode steps: 504, steps per second: 153, episode reward: -191.877, mean reward: -0.381 [-100.000,  4.862], mean action: 1.746 [0.000, 3.000],  loss: 8.193641, mae: 32.950043, mean_q: 40.382385\n",
      " 21230/50000: episode: 56, duration: 4.605s, episode steps: 648, steps per second: 141, episode reward: -161.300, mean reward: -0.249 [-100.000,  6.723], mean action: 1.735 [0.000, 3.000],  loss: 7.505517, mae: 32.894508, mean_q: 40.088646\n",
      " 21687/50000: episode: 57, duration: 3.019s, episode steps: 457, steps per second: 151, episode reward: -125.136, mean reward: -0.274 [-100.000,  4.834], mean action: 1.707 [0.000, 3.000],  loss: 6.968448, mae: 32.571228, mean_q: 39.821903\n",
      " 21866/50000: episode: 58, duration: 1.256s, episode steps: 179, steps per second: 143, episode reward:  1.831, mean reward:  0.010 [-100.000, 14.617], mean action: 1.793 [0.000, 3.000],  loss: 11.948311, mae: 32.309345, mean_q: 38.809586\n",
      " 22313/50000: episode: 59, duration: 2.961s, episode steps: 447, steps per second: 151, episode reward: -136.027, mean reward: -0.304 [-100.000, 12.112], mean action: 1.761 [0.000, 3.000],  loss: 8.509294, mae: 32.437187, mean_q: 38.288204\n",
      " 22868/50000: episode: 60, duration: 3.806s, episode steps: 555, steps per second: 146, episode reward: -130.533, mean reward: -0.235 [-100.000,  4.766], mean action: 1.674 [0.000, 3.000],  loss: 6.689185, mae: 32.340878, mean_q: 37.922142\n",
      " 23060/50000: episode: 61, duration: 1.224s, episode steps: 192, steps per second: 157, episode reward: -3.988, mean reward: -0.021 [-100.000, 13.841], mean action: 1.745 [0.000, 3.000],  loss: 6.163134, mae: 32.323475, mean_q: 37.791306\n",
      " 23617/50000: episode: 62, duration: 3.833s, episode steps: 557, steps per second: 145, episode reward: -178.394, mean reward: -0.320 [-100.000,  6.158], mean action: 1.783 [0.000, 3.000],  loss: 5.432391, mae: 31.875586, mean_q: 37.016068\n",
      " 24114/50000: episode: 63, duration: 3.410s, episode steps: 497, steps per second: 146, episode reward: -149.353, mean reward: -0.301 [-100.000, 10.126], mean action: 1.654 [0.000, 3.000],  loss: 6.743186, mae: 31.967974, mean_q: 36.291985\n",
      " 24703/50000: episode: 64, duration: 4.207s, episode steps: 589, steps per second: 140, episode reward: -233.758, mean reward: -0.397 [-100.000,  5.018], mean action: 1.649 [0.000, 3.000],  loss: 6.501945, mae: 32.057781, mean_q: 35.894127\n",
      " 25229/50000: episode: 65, duration: 3.421s, episode steps: 526, steps per second: 154, episode reward: -92.831, mean reward: -0.176 [-100.000, 13.155], mean action: 1.867 [0.000, 3.000],  loss: 7.560294, mae: 31.764534, mean_q: 34.572617\n",
      " 25895/50000: episode: 66, duration: 4.587s, episode steps: 666, steps per second: 145, episode reward: -147.027, mean reward: -0.221 [-100.000,  6.496], mean action: 1.758 [0.000, 3.000],  loss: 6.063886, mae: 32.313938, mean_q: 35.495453\n",
      " 26340/50000: episode: 67, duration: 3.070s, episode steps: 445, steps per second: 145, episode reward: -94.812, mean reward: -0.213 [-100.000, 10.287], mean action: 1.885 [0.000, 3.000],  loss: 5.395101, mae: 32.049179, mean_q: 34.752312\n",
      " 26792/50000: episode: 68, duration: 3.106s, episode steps: 452, steps per second: 146, episode reward: -148.316, mean reward: -0.328 [-100.000,  5.427], mean action: 1.741 [0.000, 3.000],  loss: 7.189688, mae: 31.833687, mean_q: 34.248768\n",
      " 27660/50000: episode: 69, duration: 6.126s, episode steps: 868, steps per second: 142, episode reward: -195.265, mean reward: -0.225 [-100.000, 18.102], mean action: 1.719 [0.000, 3.000],  loss: 7.251404, mae: 31.821753, mean_q: 34.296566\n",
      " 28660/50000: episode: 70, duration: 7.598s, episode steps: 1000, steps per second: 132, episode reward: -95.082, mean reward: -0.095 [-5.008,  5.484], mean action: 1.734 [0.000, 3.000],  loss: 6.225896, mae: 31.322540, mean_q: 32.809811\n",
      " 29311/50000: episode: 71, duration: 4.424s, episode steps: 651, steps per second: 147, episode reward: -134.919, mean reward: -0.207 [-100.000, 11.716], mean action: 1.799 [0.000, 3.000],  loss: 6.111622, mae: 31.292318, mean_q: 32.855587\n",
      " 30311/50000: episode: 72, duration: 7.775s, episode steps: 1000, steps per second: 129, episode reward: -53.985, mean reward: -0.054 [-5.019,  4.639], mean action: 1.780 [0.000, 3.000],  loss: 5.585609, mae: 30.931910, mean_q: 31.708340\n",
      " 31311/50000: episode: 73, duration: 7.191s, episode steps: 1000, steps per second: 139, episode reward: -45.835, mean reward: -0.046 [-5.131,  5.027], mean action: 1.795 [0.000, 3.000],  loss: 4.412339, mae: 30.872869, mean_q: 32.020180\n",
      " 32311/50000: episode: 74, duration: 7.561s, episode steps: 1000, steps per second: 132, episode reward: -44.090, mean reward: -0.044 [-4.684,  4.728], mean action: 1.782 [0.000, 3.000],  loss: 5.039048, mae: 30.527287, mean_q: 31.830883\n",
      " 33311/50000: episode: 75, duration: 7.662s, episode steps: 1000, steps per second: 131, episode reward: 11.243, mean reward:  0.011 [-4.609,  4.548], mean action: 1.758 [0.000, 3.000],  loss: 5.751541, mae: 30.475458, mean_q: 32.215328\n",
      " 34311/50000: episode: 76, duration: 7.657s, episode steps: 1000, steps per second: 131, episode reward: -14.062, mean reward: -0.014 [-20.103, 16.464], mean action: 1.694 [0.000, 3.000],  loss: 4.691351, mae: 30.334898, mean_q: 32.912273\n",
      " 35311/50000: episode: 77, duration: 7.799s, episode steps: 1000, steps per second: 128, episode reward: -31.009, mean reward: -0.031 [-9.086, 13.614], mean action: 1.792 [0.000, 3.000],  loss: 4.670378, mae: 30.155788, mean_q: 32.847492\n",
      " 36311/50000: episode: 78, duration: 7.651s, episode steps: 1000, steps per second: 131, episode reward: -43.600, mean reward: -0.044 [-13.421,  8.191], mean action: 1.765 [0.000, 3.000],  loss: 5.339031, mae: 30.166082, mean_q: 33.826153\n",
      " 37311/50000: episode: 79, duration: 7.540s, episode steps: 1000, steps per second: 133, episode reward: -52.640, mean reward: -0.053 [-12.454, 19.668], mean action: 1.838 [0.000, 3.000],  loss: 5.377694, mae: 30.222328, mean_q: 34.232735\n",
      " 38311/50000: episode: 80, duration: 7.468s, episode steps: 1000, steps per second: 134, episode reward: -39.372, mean reward: -0.039 [-21.114, 21.294], mean action: 1.745 [0.000, 3.000],  loss: 5.125220, mae: 30.163488, mean_q: 34.243561\n",
      " 39311/50000: episode: 81, duration: 7.039s, episode steps: 1000, steps per second: 142, episode reward:  9.151, mean reward:  0.009 [-4.644,  5.493], mean action: 1.707 [0.000, 3.000],  loss: 5.571607, mae: 29.990141, mean_q: 34.779190\n",
      " 40311/50000: episode: 82, duration: 7.829s, episode steps: 1000, steps per second: 128, episode reward:  6.661, mean reward:  0.007 [-20.158, 18.612], mean action: 1.752 [0.000, 3.000],  loss: 5.353476, mae: 29.941149, mean_q: 35.292530\n",
      " 41311/50000: episode: 83, duration: 7.456s, episode steps: 1000, steps per second: 134, episode reward:  0.585, mean reward:  0.001 [-5.260,  4.991], mean action: 1.725 [0.000, 3.000],  loss: 4.860325, mae: 29.830694, mean_q: 36.231724\n",
      " 41687/50000: episode: 84, duration: 2.492s, episode steps: 376, steps per second: 151, episode reward: -28.469, mean reward: -0.076 [-100.000, 16.788], mean action: 1.721 [0.000, 3.000],  loss: 5.891916, mae: 29.994846, mean_q: 35.827152\n",
      " 42687/50000: episode: 85, duration: 7.362s, episode steps: 1000, steps per second: 136, episode reward: -24.461, mean reward: -0.024 [-17.315, 11.677], mean action: 1.769 [0.000, 3.000],  loss: 4.942823, mae: 29.823231, mean_q: 36.358311\n",
      " 43537/50000: episode: 86, duration: 6.425s, episode steps: 850, steps per second: 132, episode reward: -158.817, mean reward: -0.187 [-100.000, 12.487], mean action: 1.738 [0.000, 3.000],  loss: 5.644711, mae: 29.626328, mean_q: 36.132214\n",
      " 44537/50000: episode: 87, duration: 7.465s, episode steps: 1000, steps per second: 134, episode reward: 26.987, mean reward:  0.027 [-23.796, 25.959], mean action: 1.657 [0.000, 3.000],  loss: 5.195484, mae: 29.946308, mean_q: 36.393772\n",
      " 45537/50000: episode: 88, duration: 7.520s, episode steps: 1000, steps per second: 133, episode reward: 61.080, mean reward:  0.061 [-22.700, 24.034], mean action: 1.590 [0.000, 3.000],  loss: 4.981012, mae: 29.545511, mean_q: 35.949524\n",
      " 46386/50000: episode: 89, duration: 6.364s, episode steps: 849, steps per second: 133, episode reward: -104.999, mean reward: -0.124 [-100.000, 12.596], mean action: 1.707 [0.000, 3.000],  loss: 4.249865, mae: 29.599688, mean_q: 36.609238\n",
      " 47386/50000: episode: 90, duration: 7.250s, episode steps: 1000, steps per second: 138, episode reward: 17.646, mean reward:  0.018 [-24.125, 22.904], mean action: 1.617 [0.000, 3.000],  loss: 4.511490, mae: 29.435183, mean_q: 36.082455\n",
      " 48386/50000: episode: 91, duration: 7.616s, episode steps: 1000, steps per second: 131, episode reward: 64.700, mean reward:  0.065 [-23.294, 23.211], mean action: 1.632 [0.000, 3.000],  loss: 5.316264, mae: 29.331211, mean_q: 36.276733\n",
      " 49386/50000: episode: 92, duration: 8.122s, episode steps: 1000, steps per second: 123, episode reward: 17.989, mean reward:  0.018 [-23.315, 24.729], mean action: 1.715 [0.000, 3.000],  loss: 5.580543, mae: 28.977005, mean_q: 35.885906\n",
      "done, took 357.681 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -54.490, steps: 1000\n",
      "Episode 2: reward: -10.954, steps: 1000\n",
      "Episode 3: reward: -68.541, steps: 1000\n",
      "Episode 4: reward: -50.157, steps: 1000\n",
      "Episode 5: reward: -46.271, steps: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e331b0bb0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "ENV_NAME = 'LunarLander-v2'\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model. This is the network structure.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "# Start the training.\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "\n",
    "# Persist the model state.\n",
    "dqn.save_weights(f'dqn_{ENV_NAME}_weights.h5f', overwrite=True)\n",
    "\n",
    "# Test the model \n",
    "dqn.test(env, nb_episodes=5, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the initial observations from a reset environment. (From it's initial state)\n",
    "initial_observation = env.reset()\n",
    "# Ask the model what the next action should be in the environment's action_space. This is an index that can be passed to env.step to take the action which would in turn return a new observation which we can look through till the environment is done etc.\n",
    "next_action = dqn.forward(initial_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can take the action suggested by the model and get a observation among other things back which we can then use in a loop.\n",
    "observation, reward, done, info = env.step(dqn.forward(initial_observation))\n",
    "\n",
    "print(env.step(dqn.forward(initial_observation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained model making action decisions.\n",
    "env_name = \"LunarLander-v2\"\n",
    "session = OpenAIGymSessionForModel(env_name)\n",
    "\n",
    "session.start(dqn);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Tensorflow)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
