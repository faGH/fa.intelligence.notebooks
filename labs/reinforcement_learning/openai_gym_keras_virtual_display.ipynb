{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to Rendering an OpenAI Gym Environment Inline\n",
    "The aim of this notebook is to demistify Reinforcement Learning. To expose how simple it really is to get going. Make these systems as accessible to non-AI/ML developers as possible.\n",
    " \n",
    "### Resources\n",
    "- https://gym.openai.com/docs/#environments\n",
    "- https://keras-rl.readthedocs.io/en/latest/agents/overview/\n",
    " \n",
    "### TODO\n",
    "- Refactor this notebook into nicely abstracted utilities that we can use to spin up RL agents with ease for any environment / problem domain.\n",
    "- Create and showcase a custom environment creation and being navigated by the RL agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pandas\n",
    "!pip install urllib3\n",
    "!pip install gym\n",
    "!pip install tensorflow\n",
    "!pip install tf-agents\n",
    "!pip install keras-rl2\n",
    "!pip install plotly\n",
    "!pip install opencv-python\n",
    "!pip install opencv-contrib-python\n",
    "!pip install av\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install box2d\n",
    "!pip install pyglet\n",
    "!pip install ale-py\n",
    "!pip install pyopengl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Virtual Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIGymSessionVideo:\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "        self.frames = []\n",
    "    \n",
    "    def renderAndCapture(self, epoch_id):\n",
    "        from PIL import Image\n",
    "        import base64\n",
    "        from io import BytesIO\n",
    "\n",
    "        three_d_rgb_array = self.environment.render(mode='rgb_array')\n",
    "        image = Image.fromarray(three_d_rgb_array, 'RGB')\n",
    "        image_buffer = BytesIO()\n",
    "        image.save(image_buffer, format='PNG')\n",
    "\n",
    "        import numpy as np\n",
    "        import cv2\n",
    "        import io\n",
    "        import os\n",
    "        \n",
    "        video_fps = 30\n",
    "        video_codec = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        video_output = cv2.VideoWriter(f'{epoch_id}.mp4', video_codec, video_fps, image.size)\n",
    "\n",
    "        for frame in self.frames:\n",
    "            video_output.write(frame)\n",
    "\n",
    "        video_output.release()\n",
    "        # Convert the video to codecs web supports.\n",
    "        os.system(f\"ffmpeg -i {epoch_id}.mp4 -vcodec libx264 {epoch_id}.web.mp4\")\n",
    "        \n",
    "        self.frames = []\n",
    "        video = io.open(f'{epoch_id}.web.mp4', 'r+b').read()\n",
    "        encoded_video = base64.b64encode(video)\n",
    "        base64_video = encoded_video.decode('utf-8')\n",
    "        video_tag =f'<video controls loop autoplay width=\"250px\" height=\"200px\"><source src=\"data:video/mp4;base64,{base64_video}\" type=\"video/mp4\" /></video>'\n",
    "        \n",
    "        displayHTML(video_tag)\n",
    "    \n",
    "    def capture(self):\n",
    "        from PIL import Image\n",
    "        import base64\n",
    "        from io import BytesIO\n",
    "\n",
    "        three_d_rgb_array = self.environment.render(mode='rgb_array')\n",
    "        image = Image.fromarray(three_d_rgb_array, 'RGB')\n",
    "        image_buffer = BytesIO()\n",
    "        image.save(image_buffer, format='PNG')\n",
    "        \n",
    "        import numpy as np\n",
    "        import cv2\n",
    "        \n",
    "        im_arr = np.frombuffer(image_buffer.getvalue(), dtype=np.uint8)\n",
    "        self.frames.append(cv2.imdecode(im_arr, flags=cv2.IMREAD_COLOR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIGymSession:\n",
    "    def __init__(self, environment_name):\n",
    "        self.environment_name = environment_name\n",
    "    \n",
    "def start(self, episodes, max_epochs=-1):\n",
    "    import gym\n",
    "    import uuid\n",
    "\n",
    "    session_id = uuid.uuid4()\n",
    "    env = gym.make(self.environment_name)\n",
    "    env_display = OpenAIGymSessionVideo(env)\n",
    "\n",
    "    # For each iteration we want to run.\n",
    "    for episode in range(episodes):\n",
    "        env.reset()\n",
    "        \n",
    "        episodes_id = uuid.uuid4()\n",
    "        current_epoch = 0\n",
    "        # Take an initial random action / step.\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # Run the loop again if the environment is not done.\n",
    "        while not(done):\n",
    "            current_epoch += 1\n",
    "\n",
    "            # Break out of the loop if we have reached max_epochs with no done status.\n",
    "            if max_epochs > -1 and current_epoch >= max_epochs:\n",
    "                # Render last image of this iteration.\n",
    "                env_display.renderAndCapture(episodes_id)\n",
    "                return\n",
    "        \n",
    "            # Take next action / step.\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            env_display.capture()\n",
    "            \n",
    "        # Render last image of this iteration.\n",
    "        env_display.renderAndCapture(episodes_id)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIGymSessionForModel:\n",
    "    def __init__(self, environment_name):\n",
    "        self.environment_name = environment_name\n",
    "    \n",
    "    def start(self, model, episodes=1, max_epochs=-1):\n",
    "        import gym\n",
    "        import uuid\n",
    "\n",
    "        session_id = uuid.uuid4()\n",
    "        env = gym.make(self.environment_name)\n",
    "        env_display = OpenAIGymSessionVideo(env)\n",
    "\n",
    "        # For each iteration we want to run.\n",
    "        for episode in range(episodes):\n",
    "            initial_observation = env.reset()\n",
    "        \n",
    "            episodes_id = uuid.uuid4()\n",
    "            current_epoch = 0\n",
    "            # Take an initial random action / step.\n",
    "            action = model.forward(initial_observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            # Run the loop again if the environment is not done.\n",
    "            while not(done):\n",
    "                current_epoch += 1\n",
    "\n",
    "                # Break out of the loop if we have reached max_epochs with no done status.\n",
    "                if max_epochs > -1 and current_epoch >= max_epochs:\n",
    "                    # Render last image of this iteration.\n",
    "                    env_display.renderAndCapture(episodes_id)\n",
    "                    return\n",
    "                \n",
    "                # Take next action / step.\n",
    "                action = model.forward(observation)\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                env_display.capture()\n",
    "\n",
    "        # Render last image of this iteration.\n",
    "        env_display.renderAndCapture(episodes_id)\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment\n",
    "Our utilities support rendering various OpenAI Gym environments. See https://gym.openai.com/envs/#classic_control for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1;\n",
    "env_name = \"LunarLander-v2\"\n",
    "session = OpenAIGymSession(env_name)\n",
    "\n",
    "session.start(episodes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1\n",
    "env_name = \"CartPole-v1\"\n",
    "session = OpenAIGymSession(env_name)\n",
    "\n",
    "session.start(episodes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# Simple environment taking a discrete and continuious action.\n",
    "class AdditionCompetitionEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AdditionCompetitionEnv, self).__init__()\n",
    "\n",
    "        self.state = 0\n",
    "        # 0) Sutract 1, 1) Add 1\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.state -= 1\n",
    "        elif action == 2:\n",
    "            self.state += 1\n",
    "        \n",
    "        done = self.state <= 200 or self.state >= 200\n",
    "        info = {}\n",
    "        \n",
    "        return self.state, self.state, done, info\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        #self.state = 0\n",
    "        \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducting a Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "ENV_NAME = 'LunarLander-v2'\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "env = AdditionCompetitionEnv()\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model. This is the network structure.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "# Start the training.\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "\n",
    "# Persist the model state.\n",
    "dqn.save_weights(f'dqn_{ENV_NAME}_weights.h5f', overwrite=True)\n",
    "\n",
    "# Test the model \n",
    "dqn.test(env, nb_episodes=5, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the initial observations from a reset environment. (From it's initial state)\n",
    "initial_observation = env.reset()\n",
    "# Ask the model what the next action should be in the environment's action_space. This is an index that can be passed to env.step to take the action which would in turn return a new observation which we can look through till the environment is done etc.\n",
    "next_action = dqn.forward(initial_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can take the action suggested by the model and get a observation among other things back which we can then use in a loop.\n",
    "observation, reward, done, info = env.step(dqn.forward(initial_observation))\n",
    "\n",
    "print(env.step(dqn.forward(initial_observation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained model making action decisions.\n",
    "env_name = \"LunarLander-v2\"\n",
    "session = OpenAIGymSessionForModel(env_name)\n",
    "\n",
    "session.start(dqn);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (TF - CUDA)",
   "language": "python",
   "name": "tensorflow-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
