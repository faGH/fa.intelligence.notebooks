{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crypto Trading Environment\n",
    "### Overview\n",
    "A reinforcement learning environment for trading crypto pairs. This environment is targeted at trading a single pair. In future iterations of this environment, we will explore allowing the trading of multiple pairs on the same environment.\n",
    "### Features\n",
    "- Allows for features / indicators / additional numerical data in the observation space.\n",
    "- Memory allows for the current timestep's observation space to have X length of historical data most recent to the current timestep.\n",
    "- Long-only positions as this is common in the crypto-world.\n",
    "- When buying stakes, max_stake_count of stakes are allowed however when selling, the environment sells all stakes at once.\n",
    "### References\n",
    "|Reference|Relevance|\n",
    "|--|--|\n",
    "|[OpenAI Gym Base](https://github.com/openai/gym/blob/master/gym/core.py)|The base class for our environment. This interface seems to be a standard in the reinforcement learning space.|\n",
    "|[AnyTrading Foundation](https://github.com/AminHP/gym-anytrading)|We used AnyTrading's work as a reference point for creating our own environment that's a little more talored to our needs.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from tensorforce import Agent, Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data\n",
    "We use the lab we worked on before for market_trading > parse_market_data to get the latest market information from FrostAura Plutus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = './data/featurized_market_data.p'\n",
    "\n",
    "with open(model_file_path, 'rb') as fp:\n",
    "    featurized_market_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data\n",
    "- Price Movement Data\n",
    "  - Time\n",
    "  - Open\n",
    "  - Close\n",
    "  - High\n",
    "  - Low\n",
    "  - Volume\n",
    "- Feature Data\n",
    "  - Indicators \n",
    "  - Features\n",
    "  - Balances\n",
    "  - Stakes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_name = 'AAVE_BTC'\n",
    "pair_data = featurized_market_data[pair_name]\n",
    "\n",
    "# This dataset contains open, close, high, low, volume and any numerical features / indicators you like.\n",
    "price_movement_df = pair_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "This refers to the decisions / actions that can be applied to the environment. Usually decided by some intelligent system like a neural network or a state vector machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(Enum):\n",
    "    Hold = 0\n",
    "    Buy = 1\n",
    "    Sell = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Movement\n",
    "This refers to the columns we can expect to be present always in the data. Open, close, high, low, volume and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceMovementColumns(Enum):\n",
    "    Time = 'time'\n",
    "    Open = 'open'\n",
    "    High = 'high'\n",
    "    Low = 'low'\n",
    "    Close = 'close'\n",
    "    Volume = 'volume'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions\n",
    "This refers to all the columns that we can expect to see in the transactions dataframe. This allows us to track all buys and sells over time. This dataframe is append-only and so we resort to joins to determine open transactions. This helps us in the long run to visualize performance and extend the enviroment's behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionsColumns(Enum):\n",
    "    PairName = 'pair_name'\n",
    "    Time = 'time_of_transaction'\n",
    "    Price = 'price_at_transaction_time'\n",
    "    Quantity = 'quantity'\n",
    "    IsBuy = 'is_buy_order'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Internal Observations\n",
    "These observations are appended to the data provided to the environment as to keep track of the environment's internal state and expose that to any wrapping model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalColumns(Enum):\n",
    "    OpenTransactionCount = '__open_transaction_count__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoTradingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, data, pair_name, max_stake_count=1, memory_window_size=50, seed=None, trading_fee_percentage=0.001):\n",
    "        assert data.ndim == 2, 'The price movement & features dataframe can only be an array of 2 dimensions (tabular).'\n",
    "        assert max_stake_count > 0, 'The max allowed stake units should be 1 or more. This allows for position stacking on a given pair. Stake amount for the first transaction would implicitly be total_profit/max_stake_count.'\n",
    "        assert memory_window_size > 0, 'The memory window should be 1 for no memory (only the most recent 1) or a positive number for a length of historical events to keep.'\n",
    "        assert data.shape[0] > memory_window_size, 'The provided data has to contain at least as many records as the length of the memory window.'\n",
    "        assert trading_fee_percentage > 0 and trading_fee_percentage < 1, 'A valid trading fee is required. Usually around 0.003 (0.3%).'\n",
    "        \n",
    "        self.seed(seed)\n",
    "        \n",
    "        # Persist locals.\n",
    "        self.pair_name = pair_name\n",
    "        # TODO: Remove tail after testing.\n",
    "        self.data = self.__extend_data_columns_and_default_values__(data.fillna(0)).tail(1100)\n",
    "        self.max_stake_count = max_stake_count\n",
    "        self.memory_window_size = memory_window_size\n",
    "        self.trading_fee_percentage = trading_fee_percentage\n",
    "        self.memory_shape = (self.memory_window_size, self.data.drop(PriceMovementColumns.Time.value, axis=1).shape[1])\n",
    "        \n",
    "        # Define spaces. This can be thought of the input and output of any model we build around this environment.\n",
    "        self.action_space = spaces.Discrete(len(Actions))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=self.memory_shape, dtype=np.float32)\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def __extend_data_columns_and_default_values__(self, df):\n",
    "        result = df.copy()\n",
    "        \n",
    "        for internal_column in InternalColumns:\n",
    "            result[internal_column.value] = 0\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def __take_action__(self, action):\n",
    "        current_timestep = self.current_window[-1:]\n",
    "        open_transactions = self.__get_open_transactions__()\n",
    "        open_transactions_count = open_transactions.shape[0]\n",
    "        penalty_for_invalid_transaction = 0\n",
    "        \n",
    "        if Actions(action) == Actions.Buy:\n",
    "            # Only allow for max_stake_count purchases.\n",
    "            positions_left_to_buy = self.max_stake_count - open_transactions_count\n",
    "            \n",
    "            if positions_left_to_buy > 0:\n",
    "                # Determine the quantity for a stake given how many stakes are still available.\n",
    "                stake_amount = self.balance / positions_left_to_buy\n",
    "                \n",
    "                # Make purchase.\n",
    "                self.transactions = self.transactions.append({\n",
    "                    TransactionsColumns.PairName.value: self.pair_name,\n",
    "                    TransactionsColumns.Time.value: current_timestep[PriceMovementColumns.Time.value].values[0],\n",
    "                    TransactionsColumns.Price.value: current_timestep[PriceMovementColumns.Close.value].values[0],\n",
    "                    TransactionsColumns.Quantity.value: stake_amount,\n",
    "                    TransactionsColumns.IsBuy.value: True\n",
    "                }, ignore_index=True)\n",
    "                # Adjust base pair balance.\n",
    "                self.balance -= stake_amount\n",
    "                \n",
    "                # Reward for taking an action.\n",
    "                penalty_for_invalid_transaction += 1\n",
    "            else:\n",
    "                # If a buy is attempted but there are no more allowed transactions, penalize the action. # TODO: Experiment with this. Lower the penalty. Take it away. Etc.\n",
    "                penalty_for_invalid_transaction =- 1\n",
    "        \n",
    "        if Actions(action) == Actions.Sell:\n",
    "            # Only sell when we have any open transactions.\n",
    "            if open_transactions_count > 0:\n",
    "                current_price = current_timestep[PriceMovementColumns.Close.value].values[0]\n",
    "                \n",
    "                # Sell all transactions at once.\n",
    "                for index, row in open_transactions.iterrows():\n",
    "                    quantity = row[TransactionsColumns.Quantity.value]\n",
    "                    \n",
    "                    # Calculate the profit for the respective transaction and add it to the running profit in the environment.\n",
    "                    self.balance += current_price * quantity\n",
    "                    # Make sell.\n",
    "                    self.transactions = self.transactions.append({\n",
    "                        TransactionsColumns.PairName.value: self.pair_name,\n",
    "                        TransactionsColumns.Time.value: current_timestep[PriceMovementColumns.Time.value].values[0],\n",
    "                        TransactionsColumns.Price.value: current_timestep[PriceMovementColumns.Close.value].values[0],\n",
    "                        TransactionsColumns.Quantity.value: quantity,\n",
    "                        TransactionsColumns.IsBuy.value: False\n",
    "                    }, ignore_index=True)\n",
    "                \n",
    "                # Reward for taking an action.\n",
    "                penalty_for_invalid_transaction += 1\n",
    "            else:\n",
    "                # If a sell is attempted but there are no open transactions, penalize the action.\n",
    "                penalty_for_invalid_transaction =- 1\n",
    "        \n",
    "        # Refresh open transactions after altering them.\n",
    "        if Actions(action) != Actions.Hold:\n",
    "            open_transactions = self.__get_open_transactions__()\n",
    "        \n",
    "        # On a hold, we simply return the rewards without changing the purchases data.\n",
    "        return (self.__determine_reward__(open_transactions) + penalty_for_invalid_transaction), open_transactions\n",
    "    \n",
    "    def __determine_reward__(self, open_transactions):\n",
    "        # Determine the reward for the balance (-1 that we started with).\n",
    "        balance_reward = self.balance - 1\n",
    "        \n",
    "        # Offset that balance by the profits of open transactions.\n",
    "        open_transactions_profits_reward = 0\n",
    "        current_price = self.current_window[-1:][PriceMovementColumns.Close.value].values[0]\n",
    "        \n",
    "        for index, open_transaction in open_transactions.iterrows():\n",
    "            ot_buy_price = open_transaction[TransactionsColumns.Price.value]\n",
    "            ot_qty = open_transaction[TransactionsColumns.Quantity.value]\n",
    "            # Calculate the different in price from when we purchased it. Multiply by how much we have to get total profit. Divide by allowed parallel transactions to normalize the reward cross-configuration.\n",
    "            ot_profit = ((current_price - ot_buy_price) * ot_qty / self.max_stake_count)\n",
    "            open_transactions_profits_reward += ot_profit\n",
    "            \n",
    "        open_transactions_profits_reward *= 10000\n",
    "        \n",
    "        # Update the episode's total running reward.\n",
    "        # Simple reward. TODO: Reward shaping.\n",
    "        reward = 1 if balance_reward > 0 else 0#balance_reward + open_transactions_profits_reward\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def __determine_observation__(self, open_transactions):\n",
    "        # Update the stakes on the most recent record / last record in the current window.\n",
    "        # Configure internal features.\n",
    "        stakes_open = open_transactions.shape[0]\n",
    "        # Perform the update on the entire dataframe as well as the current window.\n",
    "        #self.data.at[self.current_window_end_index, InternalColumns.OpenTransactionCount.value] = stakes_open\n",
    "        #self.current_window.at[self.current_window.shape[0] - 2, InternalColumns.OpenTransactionCount.value] = stakes_open\n",
    "        \n",
    "        result = self.current_window.drop(PriceMovementColumns.Time.value, axis=1)\n",
    "\n",
    "        assert result.shape == self.observation_space.shape, f'[STEP {self.current_step}]: The shape of the current observation {result.shape} is different from the observation_space shape {self.observation_space.shape}.'\n",
    "\n",
    "        return result.to_numpy()\n",
    "    \n",
    "    def __determine_done__(self, open_transactions):\n",
    "        if self.current_window.shape[0] < self.memory_window_size:\n",
    "            print('Ran out of time. Environment is now done.')\n",
    "            return True\n",
    "\n",
    "        open_transaction_count = open_transactions.shape[0]\n",
    "        \n",
    "        if self.balance <= 0 and open_transaction_count <= 0:\n",
    "            print(f'Ran out of capital with {open_transaction_count} remaining open transactions. Environment is now done.')\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def __get_step_info__(self, open_transactions):\n",
    "        return dict(\n",
    "            total_reward = self.total_reward,\n",
    "            balance = self.balance,\n",
    "            open_transactions = open_transactions\n",
    "        )\n",
    "        \n",
    "    def __next_timestep__(self):\n",
    "        self.current_window_start_index += 1\n",
    "        self.current_window_end_index += 1\n",
    "        self.current_window = self.data[self.current_window_start_index:self.current_window_end_index]\n",
    "    \n",
    "    def __get_open_transactions__(self):\n",
    "        all_transactions = self.transactions\n",
    "        sell_transaction_indexes = all_transactions.index[all_transactions[TransactionsColumns.IsBuy.value] == False]\n",
    "        last_sold_transaction_index = sell_transaction_indexes[-1] if len(sell_transaction_indexes) > 0 else -1\n",
    "        first_open_transaction_index = last_sold_transaction_index + 1\n",
    "        open_transactions = all_transactions[first_open_transaction_index:]\n",
    "\n",
    "        return open_transactions\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        step_reward, open_transactions = self.__take_action__(action)\n",
    "        observation = self.__determine_observation__(open_transactions)\n",
    "        info = self.__get_step_info__(open_transactions)\n",
    "        \n",
    "        self.__next_timestep__()\n",
    "        \n",
    "        done = self.__determine_done__(open_transactions)\n",
    "        \n",
    "        return observation, step_reward, done, info\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        \n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.data = self.__extend_data_columns_and_default_values__(self.data)\n",
    "        self.current_window_start_index = 0\n",
    "        self.current_window_end_index = self.memory_window_size\n",
    "        self.current_window = self.data[self.current_window_start_index:self.current_window_end_index]\n",
    "        self.total_reward = 0.\n",
    "        self.balance = 1.\n",
    "        self.transactions = pd.DataFrame(columns=[ ck.value for ck in TransactionsColumns ])\n",
    "\n",
    "        return self.current_window.drop(PriceMovementColumns.Time.value, axis=1).to_numpy()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        assert mode == 'human', 'Currently only human-mode is supported for rendering with rgb_array support coming.'\n",
    "        \n",
    "        print(f'Rendering the environment results now. {self.data.shape[0]} close price points and {self.transactions.shape[0]} transaction points. This may take a while.')\n",
    "        plt.cla()\n",
    "        plt.plot(self.data[PriceMovementColumns.Time.value], self.data[PriceMovementColumns.Close.value])\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Close Price')\n",
    "        \n",
    "        # Draw an indicator for all transactions.\n",
    "        for index, transaction in self.transactions.iterrows():\n",
    "            color = 'green' if transaction[TransactionsColumns.IsBuy.value] == True else 'red'\n",
    "            tick = transaction[TransactionsColumns.Time.value]\n",
    "            close = transaction[TransactionsColumns.Price.value]\n",
    "            plt.scatter(tick, close, color=color)\n",
    "            \n",
    "        # If the account was depleted, render an indicator for when that happened.\n",
    "        if self.balance <= 0.0:\n",
    "            last_transaction = self.transactions[-1:]\n",
    "            last_transaction_time = last_transaction[TransactionsColumns.Time.value].values[0]\n",
    "            last_transaction_price = last_transaction[TransactionsColumns.Price.value].values[0]\n",
    "            plt.axvline(last_transaction_time, last_transaction_price, color='red')\n",
    "        \n",
    "        plt.title(self.pair_name)\n",
    "        plt.suptitle(\n",
    "            f'Total Reward: {self.total_reward} ~ Total Profit: {round((self.balance - 1) * 100, 2)}% ~ Steps: {self.current_window_start_index + 1}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CryptoTradingEnv(pair_data, pair_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions\n",
    "A few basic tests to assure the environment is still behaving as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert env.current_window.copy().drop(PriceMovementColumns.Time.value, axis=1).shape == env.observation_space.shape, f'The observation space \"{env.observation_space.shape}\" should be the same size as the window of historical observations \"{env.current_window.shape}\".'\n",
    "assert env.transactions.shape[1] == len(TransactionsColumns) + len(InternalColumns) - 1, 'The auto-generated transactions dataframe should have all columns from the transactions columns enum.'\n",
    "assert env.reset().shape == env.observation_space.shape, f'The observation_space shape has changed after the reset. This is not allowed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-end Random Agent Result\n",
    "We take a bunch of random actions until the time or capital runs out and render the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Utility\n",
    "def visualize_episodes(rewards, transaction_counts, balances, max_reward=0, max_balance=1.0, max_transactions=0):\n",
    "    # Visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "    plt.xlabel('Episode')\n",
    "    \n",
    "    ax1.plot(transaction_counts, color='magenta')\n",
    "    ax1.set_ylabel('Actions')\n",
    "    ax1.set_xticklabels([])\n",
    "    \n",
    "    ax2.plot(balances, color='indigo')\n",
    "    ax2.set_ylabel('Balance')\n",
    "    ax2.set_xticklabels([])\n",
    "    \n",
    "    ax3.plot(rewards, color='dodgerblue')\n",
    "    ax3.set_ylabel('Rewards')\n",
    "    ax3.set_xlabel('Episodes')\n",
    "    ax3.set_xticklabels([])\n",
    "    \n",
    "    # Horizontal Lines\n",
    "    if max_reward != 0:\n",
    "        plt.axhline(max_reward, color='dodgerblue')\n",
    "        \n",
    "    if max_balance != 0:\n",
    "        plt.axhline(max_balance, color='indigo')\n",
    "        \n",
    "    if max_transactions != 0:\n",
    "        plt.axhline(max_transactions, color='magenta')\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episode diagnostics.\n",
    "episode_count = 1\n",
    "episode_rewards = []\n",
    "episode_transaction_count = []\n",
    "episode_balances = []\n",
    "episode_times = []\n",
    "\n",
    "env = CryptoTradingEnv(pair_data, pair_name, max_stake_count=1, memory_window_size=24)\n",
    "\n",
    "for ei in range(episode_count):\n",
    "    start_time = time.time()\n",
    "    done = False\n",
    "    states = env.reset()\n",
    "    step = 0\n",
    "    \n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = env.action_space.sample()\n",
    "        states, reward, done, info = env.step(action)\n",
    "\n",
    "    duration = round(time.time() - start_time, 3)\n",
    "    episode_times.append(duration)\n",
    "    episode_rewards.append(env.total_reward)\n",
    "    episode_transaction_count.append(env.transactions.shape[0])\n",
    "    episode_balances.append(env.balance)\n",
    "    \n",
    "    #visualize_episodes(episode_rewards, episode_transaction_count, episode_balances)\n",
    "    #env.render()\n",
    "    print(f'Episode {ei + 1}/{episode_count}, Time: {duration} Reward: {env.total_reward}. Balance: {env.balance}. Transaction Count: {env.transactions.shape[0]}')\n",
    "    \n",
    "plt.plot(episode_times)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intelligent Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensorforce_agent(env_name, gym_environment, model_path='./data/models/tf.{}.{}.tensorforce', force_create=False):\n",
    "    normalized_path = model_path.format(env_name.lower(), env_name.lower().replace(' ','_'))\n",
    "    environment = Environment.create(environment=gym_environment)\n",
    "    does_model_exist = os.path.exists(normalized_path)\n",
    "    agent = None\n",
    "    \n",
    "    if does_model_exist and not force_create:\n",
    "        print(f'Loading existing model.')\n",
    "        agent = Agent.load(directory=normalized_path, format='checkpoint', environment=environment)\n",
    "    else:\n",
    "        print(f'No directory \"{normalized_path}\" exists. Creating a new model.')\n",
    "\n",
    "        agent = Agent.create(\n",
    "            saver=dict(\n",
    "                directory=normalized_path,\n",
    "                frequency=50,\n",
    "                max_checkpoints=5\n",
    "            ),\n",
    "            agent='tensorforce', \n",
    "            environment=environment, \n",
    "            update=64,\n",
    "            optimizer=dict(optimizer='adam', learning_rate=1e-3),\n",
    "            objective='policy_gradient', \n",
    "            memory=15000,\n",
    "            reward_estimation=dict(horizon=20)\n",
    "        )\n",
    "    \n",
    "    return agent, environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_ddqn_agent(env_name, gym_environment, model_path='./data/models/tf.{}.{}.ddqn', force_create=False):\n",
    "    normalized_path = model_path.format(env_name.lower(), env_name.lower().replace(' ','_'))\n",
    "    environment = Environment.create(environment=gym_environment)\n",
    "    does_model_exist = os.path.exists(normalized_path)\n",
    "    agent = None\n",
    "    \n",
    "    if does_model_exist and not force_create:\n",
    "        print(f'Loading existing model.')\n",
    "        agent = Agent.load(directory=normalized_path, format='checkpoint', environment=environment)\n",
    "    else:\n",
    "        print(f'No directory \"{normalized_path}\" exists. Creating a new model.')\n",
    "        agent = Agent.create(\n",
    "            saver=dict(\n",
    "                directory=normalized_path,\n",
    "                frequency=50,\n",
    "                max_checkpoints=5\n",
    "            ),\n",
    "            agent='double_dqn',\n",
    "            environment=environment,\n",
    "            memory=15000,\n",
    "            batch_size=10000\n",
    "        )\n",
    "    \n",
    "    return agent, environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_duelingdqn_agent(env_name, gym_environment, model_path='./data/models/tf.{}.{}.duelingdqn', force_create=False):\n",
    "    normalized_path = model_path.format(env_name.lower(), env_name.lower().replace(' ','_'))\n",
    "    environment = Environment.create(environment=gym_environment)\n",
    "    does_model_exist = os.path.exists(normalized_path)\n",
    "    agent = None\n",
    "    \n",
    "    if does_model_exist and not force_create:\n",
    "        print(f'Loading existing model.')\n",
    "        agent = Agent.load(directory=normalized_path, format='checkpoint', environment=environment)\n",
    "    else:\n",
    "        print(f'No directory \"{normalized_path}\" exists. Creating a new model.')\n",
    "        agent = Agent.create(\n",
    "            saver=dict(\n",
    "                directory=normalized_path,\n",
    "                frequency=50,\n",
    "                max_checkpoints=5\n",
    "            ),\n",
    "            agent='dueling_dqn',\n",
    "            environment=environment,\n",
    "            memory=15000,\n",
    "            batch_size=10000,\n",
    "            parallel_interactions=8,\n",
    "            max_episode_timesteps=10000\n",
    "        )\n",
    "    \n",
    "    return agent, environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorForceOpenAIGymWrapper(Environment):\n",
    "    def __init__(self, open_ai_gym_env):\n",
    "        self.open_ai_gym_env = open_ai_gym_env\n",
    "        super().__init__()\n",
    "\n",
    "    def states(self):\n",
    "        return dict(type='float', shape=self.open_ai_gym_env.observation_space.shape)\n",
    "\n",
    "    def actions(self):\n",
    "        space = self.open_ai_gym_env.action_space\n",
    "        \n",
    "        if isinstance(space, gym.spaces.Discrete):\n",
    "            return dict(type='int', shape=(), num_values=space.n)\n",
    "        \n",
    "        raise Exception('Only discrete values are supported. To Add support for others as needed, see: https://github.com/tensorforce/tensorforce/blob/master/tensorforce/environments/openai_gym.py')\n",
    "\n",
    "    def reset(self):\n",
    "        return self.open_ai_gym_env.reset()\n",
    "\n",
    "    def execute(self, actions):\n",
    "        next_state, reward, terminal, info = self.open_ai_gym_env.step(actions)\n",
    "\n",
    "        return next_state, terminal, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CryptoTradingEnv(pair_data, pair_name, max_stake_count=1, memory_window_size=24)\n",
    "#tf_agent, tf_environment = create_tensorforce_agent(pair_name, TensorForceOpenAIGymWrapper(env), force_create=True)\n",
    "#tf_agent, tf_environment = create_tf_ddqn_agent(pair_name, TensorForceOpenAIGymWrapper(env), force_create=True)\n",
    "tf_agent, tf_environment = create_tf_duelingdqn_agent(pair_name, TensorForceOpenAIGymWrapper(env), force_create=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorforce = True\n",
    "episode_rewards = [0]\n",
    "episode_transaction_count = [0]\n",
    "episode_balances = [1.0]\n",
    "highest_reward = -99999.0\n",
    "highest_balance = 1.0\n",
    "highest_transaction_count = 0\n",
    "\n",
    "if tensorforce:\n",
    "    env = tf_environment.open_ai_gym_env\n",
    "    episode_count = 47700\n",
    "\n",
    "    for ei in range(episode_count):\n",
    "        start_time = time.time()\n",
    "        done = False\n",
    "        states = tf_environment.reset()\n",
    "        \n",
    "        while not done:\n",
    "            actions = tf_agent.act(states=states)\n",
    "            states, done, reward = tf_environment.execute(actions=actions)\n",
    "            tf_agent.observe(terminal=done, reward=reward)\n",
    "            \n",
    "        highest_reward = max(highest_reward, env.total_reward)\n",
    "        highest_balance = max(highest_balance, env.balance)\n",
    "        highest_transaction_count = max(highest_transaction_count, env.transactions.shape[0])\n",
    "\n",
    "        end_time = time.time() - start_time\n",
    "        episode_rewards.append(env.total_reward)\n",
    "        episode_transaction_count.append(env.transactions.shape[0])\n",
    "        episode_balances.append(env.balance)\n",
    "        \n",
    "        visualize_episodes(episode_rewards, episode_transaction_count, episode_balances, highest_reward)\n",
    "        print(f'Episode {ei + 1}/{episode_count} (Time: {round(end_time,2)}sec) Reward: {round(env.total_reward, 2)} (H. {round(highest_reward, 2)}). Balance: {env.balance} (H. {highest_balance}). Transaction Count: {env.transactions.shape[0]} (H. {highest_transaction_count})')\n",
    "\n",
    "tf_environment.open_ai_gym_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tensorforce:\n",
    "    states = tf_environment.reset()\n",
    "\n",
    "    while not done:\n",
    "        actions = tf_agent.act(states=states, independent=True)\n",
    "        states, done, reward = tf_environment.execute(actions=actions)\n",
    "\n",
    "    print(f'Evaluation Reward: {env.total_reward}. Balance: {env.balance}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c96ffff15f49694e20d6af92a59f54c1cf6ff4da4eb0cf9141168fba41748bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9 (Tensorflow)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
