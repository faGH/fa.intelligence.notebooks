{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrostAura Plutus\n",
    "### Mark 10\n",
    "This iteration of the decision engine will consist of neural network-based architecture. A Deep Q-learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from pickle generated by the ./parse_market_data notebook.\n",
    "import pickle\n",
    "\n",
    "model_file_path = './data/featurized_market_data.p'\n",
    "\n",
    "with open(model_file_path, 'rb') as fp:\n",
    "    featurized_market_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the pair we will work with for testing.\n",
    "pair_name = 'AAVE_BTC'\n",
    "price_movement_df = featurized_market_data[pair_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./fa.intelligence.notebooks/utilities/reinforcement_learning/environments/crypto_pair_trading_environment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_of_memory = 24\n",
    "env = CryptoPairTradingEnv(price_movement_df, pair_name, max_stake_count=1, memory_window_size=hours_of_memory)\n",
    "states = env.reset()\n",
    "done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a Deep Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental dependencies.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorforce dependencies.\n",
    "from tensorforce import Agent, Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a TensorForce agent and environment wrapper.\n",
    "def create_tensorforce_agent(env_name, gym_environment, model_path='./data/models/tf.{}.{}'):\n",
    "    gym_environment.reset()\n",
    "    normalized_path = model_path.format(env_name.lower(), env_name.lower().replace(' ','_'))\n",
    "    environment = Environment.create(environment=gym_environment)\n",
    "    does_model_exist = os.path.exists(normalized_path)\n",
    "    agent = None\n",
    "    \n",
    "    if does_model_exist:\n",
    "        print(f'Loading existing model.')\n",
    "        agent = Agent.load(directory=normalized_path, format='checkpoint', environment=environment)\n",
    "    else:\n",
    "        print(f'No directory \"{normalized_path}\" exists. Creating a new model.')\n",
    "\n",
    "        agent = Agent.create(\n",
    "            saver=dict(\n",
    "                directory=normalized_path,\n",
    "                frequency=50,\n",
    "                max_checkpoints=5\n",
    "            ),\n",
    "            agent='tensorforce', \n",
    "            environment=environment, \n",
    "            update=64,\n",
    "            optimizer=dict(optimizer='adam', learning_rate=1e-3),\n",
    "            objective='policy_gradient', \n",
    "            memory=15000,\n",
    "            reward_estimation=dict(horizon=20)\n",
    "        )\n",
    "    \n",
    "    return agent, environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorForce agent and environment wrapper. \n",
    "tf_agent, tf_environment = create_tensorforce_agent(pair_name, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 100\n",
    "\n",
    "for ei in range(episode_count):\n",
    "    states = tf_environment.reset()\n",
    "    \n",
    "    while not done:\n",
    "        actions = tf_agent.act(states=states)\n",
    "        states, done, reward = tf_environment.execute(actions=actions)\n",
    "        tf_agent.observe(terminal=done, reward=reward)\n",
    "\n",
    "    print(f'Episode {ei + 1} Reward: {env.total_reward}. Balance: {env.balance}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = tf_environment.reset()\n",
    "\n",
    "while not done:\n",
    "    actions = tf_agent.act(states=states, independent=True)\n",
    "    states, done, reward = tf_environment.execute(actions=actions)\n",
    "\n",
    "print(f'Evaluation {ei + 1} Reward: {env.total_reward}. Balance: {env.balance}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next\n",
    "### Mark 11\n",
    "Add another dimension to the observable space to have all pairs available as to allow the agent to learn causal relationships between pairs. For example if ETH dips, BTC has a certain probability to respond to that in a certain way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Tensorflow)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
