{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bifrost Gradient Booster\n",
    "A generic autonomous model builder for various problems using gradient boosting.\n",
    "\n",
    "Problem Areas:\n",
    "- Regression\n",
    "- Binary Classification\n",
    "- Time Series Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (1.21.6)\n",
      "Requirement already satisfied: sklearn in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: graphviz in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (0.20)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\miniforge3\\envs\\fbprophet\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost pandas numpy sklearn graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BifrostGradientBooster:\n",
    "    is_timeseries_problem: bool = False\n",
    "    global_scaling_factor: int = 1\n",
    "    default_parameters = {\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 3,\n",
    "        'colsample_bytree': 1,\n",
    "        'subsample': 1,\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 1,\n",
    "        'random_state': 1502,\n",
    "        'eval_metric': 'rmse',\n",
    "        'objective': 'reg:squarederror'\n",
    "    }\n",
    "    tuning_parameter_range = {\n",
    "        'learning_rate':[ 0.3, 0.1, 0.03 ],\n",
    "        'max_depth': [ 2, 6, 12 ],\n",
    "        'colsample_bytree': [ 0.5, 0.75, 1 ],\n",
    "        'subsample': [ 0.5, 0.75, 1 ],\n",
    "        'min_child_weight': [ 1, 5, 15 ],\n",
    "        'gamma': [ 1 ],\n",
    "        'random_state': [ 1502 ],\n",
    "        'eval_metric': [ 'rmse' ],\n",
    "        'objective': [ 'reg:squarederror' ]\n",
    "    }\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame,\n",
    "                 column_name_to_predict: str,\n",
    "                 use_binary_classifier: bool = False,\n",
    "                 data_time_column_name: str = None,\n",
    "                 enable_global_scaling: bool = False,\n",
    "                 replace_missing_values: bool = True,\n",
    "                 drop_columns_with_no_unique_values: bool = True,\n",
    "                 replace_whitespace: bool = True):\n",
    "        self.data = data.copy()\n",
    "        self.columns_to_drop = list()\n",
    "        self.column_name_to_predict = column_name_to_predict\n",
    "        self.use_binary_classifier = use_binary_classifier\n",
    "        self.model = None\n",
    "        self.data_time_column_name = data_time_column_name\n",
    "        \n",
    "        if enable_global_scaling:\n",
    "            self.global_scaling_factor = self.__determine_common_scale__(self.data)\n",
    "            self.__apply_common_scale__(data=self.data, scale=self.global_scaling_factor)\n",
    "            \n",
    "            print(f'Global scaling has been set to {self.global_scaling_factor}.')\n",
    "            \n",
    "        if not self.data_time_column_name == None:\n",
    "            # Move all Y values one into the future as we would want to predict the future Y given a current state (X).\n",
    "            self.data[self.column_name_to_predict] = self.data[self.column_name_to_predict].shift(1, fill_value=0)\n",
    "            self.data = self.__featurize_time_from_column__(self.data, self.data_time_column_name)\n",
    "\n",
    "        if replace_missing_values:\n",
    "            self.replace_missing_values()\n",
    "            \n",
    "        if drop_columns_with_no_unique_values:\n",
    "            self.drop_columns_with_no_unique_values()\n",
    "            \n",
    "        if replace_whitespace:\n",
    "            self.replace_whitespace()\n",
    "\n",
    "    def __determine_common_scale__(self, data: pd.DataFrame):\n",
    "        maximum_common_scale = 1\n",
    "        \n",
    "        for column in data.columns:\n",
    "            column_is_long_float_type = data[column].dtype == np.float64\n",
    "            \n",
    "            if column_is_long_float_type:\n",
    "                first_column_value_str= str(data[column].values[0])\n",
    "                should_scale = len(first_column_value_str.split('.')[1]) > 1\n",
    "                maximum_local_scale = 1\n",
    "                \n",
    "                while should_scale:\n",
    "                    maximum_local_scale *= 10\n",
    "                    should_scale = len(str(data[column].values[0] * maximum_local_scale).split('.')[1]) > 1\n",
    "\n",
    "                maximum_common_scale = max(maximum_common_scale, maximum_local_scale)\n",
    "\n",
    "        return maximum_common_scale\n",
    "    \n",
    "    def __apply_common_scale__(self, data: pd.DataFrame, scale: int, reverse: bool=False):\n",
    "        for column in data.columns:\n",
    "            column_is_long_float_type = data[column].dtype == np.float64\n",
    "            \n",
    "            if column_is_long_float_type:\n",
    "                if reverse:\n",
    "                    data[column] = data[column] / scale\n",
    "                else:\n",
    "                    data[column] = data[column] * scale\n",
    "\n",
    "    def __get_training_test_dfs__(self, training_split: float):\n",
    "        training_record_count = int(len(self.data) * training_split)\n",
    "        \n",
    "        return self.data[:training_record_count], self.data[training_record_count:]\n",
    "\n",
    "    def __get_x_y_dfs__(self, df: pd.DataFrame, column_name_to_predict: str) -> pd.DataFrame:\n",
    "        x = df.drop(columns=[column_name_to_predict])\n",
    "        y = df.loc[:, [column_name_to_predict]]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def __get_df_matrix__(self, df: pd.DataFrame, column_name_to_predict: str, name: str) -> xgb.DMatrix:\n",
    "        x, y = self.__get_x_y_dfs__(df, column_name_to_predict=column_name_to_predict)\n",
    "        matrix = xgb.DMatrix(data = x, label = y)\n",
    "        \n",
    "        print(f'[{name}] Matrix X: {x.shape}, Matrix Y: {y.shape}')\n",
    "        \n",
    "        return matrix, x, y\n",
    "\n",
    "    def __calculate_mape_score__(self, y_true: pd.Series, y_pred: pd.Series):\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        \n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    def __print_model_assessment__(self, test_column_data: pd.Series, predictions: pd.Series):\n",
    "        if not self.use_binary_classifier:\n",
    "            print(f'Mean Absolute Error: {mean_absolute_error(test_column_data, predictions)} ({mean_absolute_error(test_column_data, predictions) / self.global_scaling_factor} reverse-scaled.)')\n",
    "            print(f'Mean Squared Error: {np.sqrt(mean_squared_error(test_column_data, predictions))} ({np.sqrt(mean_squared_error(test_column_data, predictions)) / self.global_scaling_factor} reverse-scaled.)')\n",
    "            print(f'MAPE: {self.__calculate_mape_score__(test_column_data, predictions)} ({self.__calculate_mape_score__(test_column_data, predictions) / self.global_scaling_factor} reverse-scaled.)')\n",
    "        else:\n",
    "            print(f'Test Data Accuracy: {accuracy_score(test_column_data, predictions)}')\n",
    "\n",
    "    def __featurize_time_from_column__(self, data: pd.DataFrame, column_name: str, column_prefix: str='t_'):\n",
    "        __data__: pd.DataFrame = data.copy()\n",
    "        \n",
    "        parsed_date_temporary_column = pd.to_datetime(__data__[column_name])\n",
    "        __data__.drop(columns=[column_name], inplace=True)\n",
    "            \n",
    "        __data__[f'{column_prefix}year'] = parsed_date_temporary_column.dt.year\n",
    "        __data__[f'{column_prefix}month'] = parsed_date_temporary_column.dt.month\n",
    "        __data__[f'{column_prefix}day'] = parsed_date_temporary_column.dt.day\n",
    "        __data__[f'{column_prefix}hour'] = parsed_date_temporary_column.dt.hour\n",
    "        __data__[f'{column_prefix}minute'] = parsed_date_temporary_column.dt.minute\n",
    "        __data__[f'{column_prefix}day_of_year'] = parsed_date_temporary_column.dt.dayofyear\n",
    "        __data__[f'{column_prefix}day_of_week'] = parsed_date_temporary_column.dt.dayofweek\n",
    "        __data__[f'{column_prefix}quarter'] = parsed_date_temporary_column.dt.quarter\n",
    "        self.is_timeseries_problem = True\n",
    "            \n",
    "        print(f'Extracted time series features from column \"{column_name}\" and dropped the original column.')\n",
    "        \n",
    "        return __data__\n",
    "\n",
    "    def drop_columns_with_no_unique_values(self):\n",
    "        self.columns_to_drop = [ c for c in self.data.columns if len(self.data[c].unique()) <= 1 ]\n",
    "        print(f'Dropping {len(self.columns_to_drop)} columns due to only containing 1 or less unique values. -> {self.columns_to_drop}')\n",
    "        self.data.drop(columns=self.columns_to_drop, inplace=True)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def replace_whitespace(self,\n",
    "                           substitute: str = '_',\n",
    "                           from_column_names: bool=True, \n",
    "                           from_values: bool=True):\n",
    "        if from_column_names:\n",
    "            self.data.columns = self.data.columns.str.replace(' ', substitute)\n",
    "        \n",
    "        if from_values:\n",
    "            self.data.replace(' ', substitute, regex=True, inplace=True)\n",
    "            \n",
    "        print(f'Whitespace replaced with \"{substitute}\".')\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def replace_missing_values(self, value: int = 0):\n",
    "        self.data.fillna(value)\n",
    "        \n",
    "        print(f'Replaced all missing values with {value}.')\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def onehot_encode_categorical_columns(self, column_names: list):\n",
    "        if len(column_names) > 0:\n",
    "            self.data = pd.get_dummies(self.data, columns=column_names)\n",
    "            print(f'One-Hot encoded {len(column_names)} columns. -> {column_names}')\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def fit(self,\n",
    "            enable_hyperparameter_optimization: bool,\n",
    "            training_split: float=0.8):\n",
    "        self.training_split = training_split\n",
    "        \n",
    "        if not self.is_timeseries_problem:\n",
    "            self.data = shuffle(self.data)\n",
    "        \n",
    "        training_df, testing_df = self.__get_training_test_dfs__(training_split=training_split)\n",
    "        training_matrix, training_x, training_y = self.__get_df_matrix__(training_df, column_name_to_predict=self.column_name_to_predict, name='Training')\n",
    "        testing_matrix, testing_x, testing_y = self.__get_df_matrix__(testing_df, column_name_to_predict=self.column_name_to_predict, name='Testing')\n",
    "        parameters_to_use = self.default_parameters\n",
    "        \n",
    "        if enable_hyperparameter_optimization:\n",
    "            grid_result = None\n",
    "            \n",
    "            if self.use_binary_classifier:\n",
    "                xgbc0 = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                          booster='gbtree',\n",
    "                                          eval_metric='auc',\n",
    "                                          tree_method='hist',\n",
    "                                          grow_policy='lossguide',\n",
    "                                          use_label_encoder=False)\n",
    "                default_params = {}\n",
    "                gparams = xgbc0.get_params()\n",
    "                \n",
    "                for key in gparams.keys():\n",
    "                    gp = gparams[key]\n",
    "                    default_params[key] = [gp]\n",
    "                \n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=xgb.XGBClassifier(),\n",
    "                    param_grid=default_params,\n",
    "                    cv=3,\n",
    "                    scoring='accuracy',\n",
    "                    verbose=1,\n",
    "                    return_train_score=True,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                grid_result = MultiOutputRegressor(grid_search).fit(training_x, training_y)\n",
    "            else:\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=xgb.XGBRegressor(),\n",
    "                    param_grid=self.tuning_parameter_range,\n",
    "                    cv=3,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                grid_result = MultiOutputRegressor(grid_search).fit(training_x, training_y)\n",
    "\n",
    "            parameters_to_use = grid_result.estimators_[0].best_params_\n",
    "            print(f'Hyperparameter optimization completed successfully.')\n",
    "            \n",
    "        self.model = xgb.train(\n",
    "            params = parameters_to_use,\n",
    "            dtrain = training_matrix,\n",
    "            num_boost_round = 1000,\n",
    "            evals = [(testing_matrix, self.column_name_to_predict)],\n",
    "            verbose_eval = 200\n",
    "        )\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def evaluate(self):\n",
    "        training_df, testing_df = self.__get_training_test_dfs__(training_split=self.training_split)\n",
    "        testing_matrix, testing_x, testing_y = self.__get_df_matrix__(testing_df, column_name_to_predict=self.column_name_to_predict, name='Testing')\n",
    "        \n",
    "        # Predict.\n",
    "        predictions = self.predict(future_data=testing_df,\n",
    "                                   bypass_scale_application=True)\n",
    "\n",
    "        if not self.use_binary_classifier:        \n",
    "            (training_df[self.column_name_to_predict] / self.global_scaling_factor)[1:].rename('Training Data').plot(figsize=(9,6), legend=True)\n",
    "            (testing_df[self.column_name_to_predict] / self.global_scaling_factor).rename('Test Data').plot(legend=True)\n",
    "            predictions.rename('Predictions').plot(legend=True)\n",
    "        else:\n",
    "            predictions = (predictions > 0.5).astype(int)\n",
    "            ConfusionMatrixDisplay.from_predictions(y_true=testing_y, y_pred=predictions)\n",
    "        \n",
    "        self.__print_model_assessment__(testing_df[self.column_name_to_predict], predictions)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self,\n",
    "                future_data: pd.DataFrame,\n",
    "                bypass_scale_application: bool = False) -> pd.Series:\n",
    "        __future_data__: pd.DataFrame = future_data.copy()\n",
    "        \n",
    "        if self.is_timeseries_problem and self.data_time_column_name in future_data.columns:\n",
    "            __future_data__ = self.__featurize_time_from_column__(__future_data__, self.data_time_column_name)\n",
    "            __future_data__.drop(columns=self.columns_to_drop, inplace=True)\n",
    "        \n",
    "        if not bypass_scale_application:\n",
    "            self.__apply_common_scale__(__future_data__, self.global_scaling_factor)\n",
    "        \n",
    "        matrix, x, y = self.__get_df_matrix__(__future_data__, column_name_to_predict=self.column_name_to_predict, name='Prediction')\n",
    "        predictions = pd.Series(self.model.predict(matrix)) / self.global_scaling_factor\n",
    "        predictions.index = __future_data__.index\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def visualize(self, verbose: bool=False):\n",
    "        if verbose:\n",
    "            for importance_type in ('weight', 'gain', 'cover', 'total_gain', 'total_cover'):\n",
    "                print(f'{importance_type}: {self.model.get_score(importance_type=importance_type)}')\n",
    "        \n",
    "        return xgb.to_graphviz(self.model, num_trees=0, size='10,10')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b9d388d1a2cdbcca94ddfeb5cb67ceea1525d82c54d57a1349322cbebc81d18"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('fbprophet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
