{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bifrost Neural Engine\n",
    "A generic autonomous model builder for various problems using neural networks.\n",
    "\n",
    "Problem Areas:\n",
    "- Regression\n",
    "- Classification\n",
    "- Time Series Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./../../utilities/common/data_loader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging as log\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BifrostNeuralEngine(nn.Module):\n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame, \n",
    "                 labels_column_names: list,\n",
    "                 autobootstrap_filename: str = None,\n",
    "                 date_column_name: str = None,\n",
    "                 layers_config: list = list(), \n",
    "                 dropout_probability: float=0.5,\n",
    "                 learning_rate: float=0.001):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_loader = DataLoader(data=data,\n",
    "                                      labels_column_names=labels_column_names,\n",
    "                                      date_column_name=date_column_name)\n",
    "        train_x, train_y, _, _ = self.data_loader.get_train_test_split()\n",
    "        self.features_column_names = train_x.columns\n",
    "        self.output_count = train_y.shape[1]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.labels_column_names = labels_column_names\n",
    "        self.autobootstrap_filename = autobootstrap_filename\n",
    "\n",
    "        # Batch normalizer.\n",
    "        self.bn_cont = nn.BatchNorm1d(len(self.features_column_names))\n",
    "\n",
    "        # Layers.\n",
    "        layers = list()\n",
    "        n_in = len(self.features_column_names)\n",
    "\n",
    "        if len(layers_config) <= 0:\n",
    "            layers_config = self.__auto_determine_layers_config__(input_feature_count=n_in)\n",
    "\n",
    "        for i in layers_config:\n",
    "            layers.append(nn.Linear(in_features=n_in, out_features=i))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.BatchNorm1d(num_features=i))\n",
    "            layers.append(nn.Dropout(p=dropout_probability))\n",
    "            n_in = i\n",
    "\n",
    "        layers.append(nn.Linear(in_features=layers_config[-1], out_features=self.output_count))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        if self.autobootstrap_filename is not None:\n",
    "            if os.path.isfile(self.autobootstrap_filename):\n",
    "                self.load_state_dict(torch.load(self.autobootstrap_filename))\n",
    "                self.data_loader.state_dict = torch.load(self.autobootstrap_filename.replace('.pt', '.data_loader.pt'))\n",
    "\n",
    "    def __auto_determine_layers_config__(self, input_feature_count: int) -> list:\n",
    "        return [\n",
    "            int(input_feature_count * 1.25),\n",
    "            int(input_feature_count * 1.10),\n",
    "            int(input_feature_count * 0.70),\n",
    "            int(input_feature_count)\n",
    "        ]\n",
    "\n",
    "    def __get_tensors_for_dataframe__(self, data: pd.DataFrame) -> torch.tensor:\n",
    "        return torch.tensor(data.values, dtype=torch.float)\n",
    "\n",
    "    def __eval_test_data__(self, test_x: torch.tensor, test_y: torch.tensor, criterion: nn.modules.loss._Loss):\n",
    "        log.info(f'Evaluating test data.')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(test_x)\n",
    "            loss = torch.sqrt(criterion(y_pred, test_y))\n",
    "\n",
    "        log.info(f'Training loss evaluation: {loss}')\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        x = self.bn_cont(x)\n",
    "        x = self.layers(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def fit(self, epochs: int):\n",
    "        start_time: int = time.time()\n",
    "        criterion: nn.modules.loss._Loss = nn.MSELoss()# if self.output_count == 1 else nn.CrossEntropyLoss()\n",
    "        optimizer: torch.optim.Optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        train_x, train_y, test_x, test_y = self.data_loader.get_train_test_split()\n",
    "        train_x = self.__get_tensors_for_dataframe__(data=train_x)\n",
    "        train_y = self.__get_tensors_for_dataframe__(data=train_y)\n",
    "        test_x = self.__get_tensors_for_dataframe__(data=test_x)\n",
    "        test_y = self.__get_tensors_for_dataframe__(data=test_y)\n",
    "        losses = list()\n",
    "        prev_displayed_loss = -1\n",
    "\n",
    "        pbar = tqdm(range(epochs))\n",
    "        \n",
    "        for i in pbar:\n",
    "            i += 1\n",
    "            y_pred = self(train_x)\n",
    "            loss = torch.sqrt(criterion(y_pred, train_y))\n",
    "            losses.append(loss)\n",
    "\n",
    "            if i % 50 == 1:\n",
    "                log.debug(f'Epoch: {i}, Loss: {loss} (Prev: {prev_displayed_loss})')\n",
    "                pbar.set_description(f'Epoch: {i}, Loss: {loss} (Prev: {prev_displayed_loss})')\n",
    "                prev_displayed_loss = loss\n",
    "                \n",
    "                if self.autobootstrap_filename is not None:\n",
    "                    torch.save(self.state_dict(), self.autobootstrap_filename)\n",
    "                    torch.save(self.data_loader.state_dict, self.autobootstrap_filename.replace('.pt', '.data_loader.pt'))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        log.info(f'Training took: {duration / 60} minutes.')\n",
    "\n",
    "        self.__eval_test_data__(test_x=test_x, test_y=test_y, criterion=criterion)\n",
    "        plt.plot(range(epochs), [float(l) for l in losses])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        x = self.data_loader.get_featurized_data(data=x)\n",
    "        x = x[[c for c in x.columns if c in self.features_column_names]]\n",
    "        x = self.__get_tensors_for_dataframe__(data=x)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.forward(x)\n",
    "            predictions = pd.DataFrame(predictions.numpy())\n",
    "            predictions.columns = self.labels_column_names\n",
    "            predictions = self.data_loader.apply_sigmoid_scale(data=predictions, reverse=True)\n",
    "            \n",
    "            return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0203f9377e450cf3e5fd498dcfe93bad69687b6515d650e7d79a42aa53323e2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
